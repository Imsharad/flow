import Foundation
import WhisperKit
import CoreML

actor WhisperKitService {
    private var whisperKit: WhisperKit?
    private var isModelLoaded = false

    private var modelName: String {
        return UserDefaults.standard.string(forKey: "GhostType.SelectedModel") ?? "openai_whisper-large-v3-turbo"
    }
    
    // ðŸ¦„ Unicorn Stack: ANE Enable Flag
    // Re-enabled for Distil-Whisper as it does not trigger the M1 Pro compiler hang
    // âš ï¸ UPDATE 2: Still hangs on Distil. Disabling ANE permanently for Large variants on M1 Pro.
    private let useANE = false
    
    init() {
        Task {
            await loadModel()
        }
    }
    
    func loadModel() async {
        let currentModel = self.modelName
        print("ðŸ¤– WhisperKitService: Loading model \(currentModel)...")
        print("ðŸ§  WhisperKitService: Compute mode = \(useANE ? "ANE (.all)" : "CPU/GPU (.cpuAndGPU)")")
        
        do {
            // Strategy B: Run in detached task to avoid actor/main-thread blocking during CoreML load
            let pipeline = try await Task.detached(priority: .userInitiated) { [currentModel, useANE] in
                
                // ðŸ¦„ Unicorn Stack: ANE compute for lowest latency
                let computeOptions: ModelComputeOptions
                if useANE {
                    computeOptions = ModelComputeOptions(
                        melCompute: .all,
                        audioEncoderCompute: .all,
                        textDecoderCompute: .all,
                        prefillCompute: .all
                    )
                    print("ðŸ§  WhisperKitService (Detached): Configured computeOptions = .all (ANE enabled)")
                } else {
                    computeOptions = ModelComputeOptions(
                        melCompute: .cpuAndGPU,
                        audioEncoderCompute: .cpuAndGPU,
                        textDecoderCompute: .cpuAndGPU,
                        prefillCompute: .cpuOnly
                    )
                    print("ðŸ¤– WhisperKitService (Detached): Configured computeOptions = .cpuAndGPU (ANE bypassed)")
                }

                // Initialize WhisperKit with compute options
                // ðŸ¦„ Unicorn Stack: Fallback Logic
                let kit: WhisperKit
                do {
                    // Try preferred options first
                    kit = try await WhisperKit(model: currentModel, computeOptions: computeOptions)
                } catch {
                    if useANE {
                        print("âš ï¸ WhisperKitService: ANE init failed: \(error). Falling back to CPU/GPU.")
                        let fallbackOptions = ModelComputeOptions(
                            melCompute: .cpuAndGPU,
                            audioEncoderCompute: .cpuAndGPU,
                            textDecoderCompute: .cpuAndGPU,
                            prefillCompute: .cpuOnly
                        )
                        kit = try await WhisperKit(model: currentModel, computeOptions: fallbackOptions)
                        print("âœ… WhisperKitService: Recovered with CPU/GPU fallback.")
                    } else {
                        // If we weren't trying ANE, it's a real error
                        throw error
                    }
                }
                
                print("ðŸ¤– WhisperKitService (Detached): Loading models...")
                try await kit.loadModels()
                
                return kit
            }.value
            
            self.whisperKit = pipeline
            self.isModelLoaded = true
            print("âœ… WhisperKitService: Model loaded & prewarmed (Detached).")
        } catch {
            print("âŒ WhisperKitService: Failed to load model: \(error)")
        }
    }
    
    /// Transcribe a buffer of audio samples.
    /// - Parameter audio: Array of Float samples (16kHz).
    /// - Parameter promptTokens: Optional tokens from previous segment to provide context.
    /// - Parameter prompt: Optional text string context (tokenized internally).
    /// - Returns: A tuple containing the transcribed text and the token IDs.
    func transcribe(audio: [Float], promptTokens: [Int]? = nil, prompt: String? = nil) async throws -> (text: String, tokens: [Int], segments: [Segment]) {
        guard let pipeline = whisperKit, isModelLoaded else {
            print("âš ï¸ WhisperKitService: Model not loaded yet")
            throw TranscriptionError.modelLoadFailed
        }
        
        // Convert text prompt to tokens if provided
        var resolvedPromptTokens = promptTokens ?? []
        if let textPrompt = prompt, !textPrompt.isEmpty, let tokenizer = pipeline.tokenizer {
             let textTokens = tokenizer.encode(text: textPrompt)
             // Prepend or append? Usually we want prompt to be the "prefix".
             // If we already have context tokens (e.g. from previous chunk), we combine.
             // Usually `promptTokens` are the *previous* transcription.
             // The `prompt` text is often a system instruction or context description.
             // WhisperKit typically treats `promptTokens` as the "tokens generated so far" (prefix).

             // If we have both, we probably want textTokens + promptTokens (or vice versa).
             // Let's assume textPrompt is "System Context" and promptTokens is "Previous Speech".
             // We prepend text tokens.
             resolvedPromptTokens = textTokens + resolvedPromptTokens
        }

        // Truncate if too long (max 224 or so)
        if resolvedPromptTokens.count > 224 {
            resolvedPromptTokens = Array(resolvedPromptTokens.suffix(224))
        }

        print("ðŸ¤– WhisperKitService: Transcribing \(audio.count) samples (Prompt Tokens: \(resolvedPromptTokens.count))...")
        let start = Date()
        
        // Tuning: Suppress hallucinations and timestamps for cleaner text
        let decodingOptions = DecodingOptions(
            verbose: true,
            task: .transcribe,
            language: "en", // Force English for now to avoid auto-detect errors on short clips
            temperature: 0.0, // Greedy decoding for stability
            temperatureFallbackCount: 0, // Fail fast on Turbo
            skipSpecialTokens: true,
            withoutTimestamps: false, // OFF: We need timestamps for Consensus
            wordTimestamps: true, // ON: We need word-level precision
            promptTokens: resolvedPromptTokens.isEmpty ? nil : resolvedPromptTokens, // Context carryover
            compressionRatioThreshold: 2.4, // Default
            logProbThreshold: -1.0, // Default
            noSpeechThreshold: 0.4 // Aggressive silence detection
        )
        
        let result = try await pipeline.transcribe(audioArray: audio, decodeOptions: decodingOptions)
        
        let duration = Date().timeIntervalSince(start)
        
        // ðŸ¦„ Unicorn Stack: Detailed latency metrics
        let audioDurationSec = Double(audio.count) / 16000.0
        let rtf = duration / audioDurationSec
        let latencyMs = duration * 1000
        
        print("ðŸ“Š WhisperKitService: Latency Metrics")
        print("   Audio: \(String(format: "%.2f", audioDurationSec))s | E2E: \(String(format: "%.0f", latencyMs))ms | RTF: \(String(format: "%.3f", rtf))x")
        print("   Target: E2E <1000ms, RTF <0.3x | Status: \(latencyMs < 1000 ? "âœ… PASS" : "âš ï¸ ABOVE TARGET")")
        
        // Combine segments and clean up artifacts
        let text = result.map { $0.text }.joined(separator: " ").trimmingCharacters(in: CharacterSet.whitespacesAndNewlines)
        
        // Collect all tokens from all segments
        let tokens = result.flatMap { $0.segments }.flatMap { $0.tokens }
        
        // Map to internal Segment struct
        let segments = result.flatMap { $0.segments }.flatMap { segment in
            let words = segment.words ?? []
            return words.map { word in
                Segment(
                    word: word.word,
                    startTime: TimeInterval(word.start),
                    endTime: TimeInterval(word.end),
                    probability: word.probability
                )
            }
        }
        
        return (text, tokens, segments)
    }
    /// Convert audio samples to Log-Mel Spectrogram using WhisperKit's internal FeatureExtractor.
    /// - Parameter audio: Array of Float samples (16kHz).
    /// - Returns: MLMultiArray of shape [1, 128, 3000] (for Large-v3).
    func audioToMel(_ audio: [Float]) async throws -> MLMultiArray? {
        guard let pipeline = whisperKit, isModelLoaded else {
            print("âš ï¸ WhisperKitService: Cannot extract features, model not loaded")
            return nil
        }
        
        // 1. Pad/Trim audio to 30s (480,000 samples)
        // FeatureExtractor.windowSamples usually refers to 30s window
        let windowSamples = pipeline.featureExtractor.windowSamples ?? 480_000
        
        // Use AudioProcessor to convert [Float] -> MLMultiArray with padding
        guard let inputAudio = AudioProcessor.padOrTrimAudio(
            fromArray: audio,
            startAt: 0,
            toLength: windowSamples,
            saveSegment: false
        ) else {
            print("âŒ WhisperKitService: Failed to pad/trim audio")
            return nil
        }
        
        // 2. Run CoreML FeatureExtractor
        // This returns (1, 128, 3000) for distil-large-v3
        let mel = try await pipeline.featureExtractor.logMelSpectrogram(fromAudio: inputAudio)
        return mel as? MLMultiArray
    }
    
    func convertTokenToId(_ token: String) async -> Int? {
        return whisperKit?.tokenizer?.convertTokenToId(token)
    }
    
    /// Convert token IDs back to text using WhisperKit's tokenizer.
    func detokenize(tokens: [Int]) async -> String {
        guard let tokenizer = whisperKit?.tokenizer else {
            return ""
        }
        return tokenizer.decode(tokens: tokens)
    }
}
