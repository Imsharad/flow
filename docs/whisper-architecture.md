Technical Audit: Distil-Whisper Large V3 Architecture and Integration Strategy for GhostType on Apple Silicon1. Executive SummaryThis comprehensive research report serves as a definitive architectural audit and implementation guide for the "GhostType" engineering team. The analysis specifically addresses the integration of the distil-whisper-large-v3 automatic speech recognition (ASR) model into a macOS Swift application utilizing the MLX framework.The investigation was prompted by three critical engineering challenges: the precise validation of tensor shape transformations within the audio encoder's convolutional stem, the forensic mapping of weight keys from the Hugging Face safetensors format to the Swift model hierarchy, and the resolution of the positional embedding mechanism (sinusoidal versus learned).1.1 Core FindingsOur analysis, grounded in a deep review of the distil-whisper architecture, the MLX framework specifications, and the underlying transformer principles, yields the following definitive conclusions:Tensor Topology and Transposition: The transformation transposed(0, 2, 1) applied to the input log-Mel spectrogram is correct and mandatory. The MLX Conv1d operator is optimized for a channels-last (NLC) configuration. The input pipeline transforms the raw spectrogram from (Batch, Channels, Time)—specifically (1, 128, 3000)—to (1, 3000, 128) before entering the convolutional stem. The stem subsequently downsamples the temporal dimension by a factor of 2 via a strided convolution, resulting in a latent representation of shape (1, 1500, 1280).Positional Embedding Mechanism: The distil-whisper-large-v3 encoder utilizes fixed sinusoidal positional embeddings, consistent with the original OpenAI Whisper architecture.1 These embeddings are procedural constants generated at runtime and are not present in the weight file. The absence of a model.encoder.embed_positions.weight key in the source file is a feature, not a bug. In contrast, the decoder utilizes learned positional embeddings which must be loaded from the weights.Weight Key Mapping: A distinct set of keys present in the source safetensors file effectively "ghost" the Swift implementation. These include redundant projection weights due to weight tying (e.g., proj_out.weight), training-specific metadata (e.g., scale parameters), and specific bias vectors that may be fused in optimized inference implementations. A detailed "deny-list" is provided to prevent initialization errors.The following report details the theoretical underpinnings, mathematical proofs, and implementation specifics for these findings.2. Architectural Context: Distil-Whisper and Apple SiliconTo understand the specific constraints of the GhostType implementation, one must first deconstruct the architectural lineage of the model and the hardware-software environment in which it operates.2.1 The Distil-Whisper ParadigmDistil-Whisper represents a significant optimization in the field of ASR, achieved through knowledge distillation. In this paradigm, a "student" model is trained to reproduce the behavior of a larger "teacher" model—in this case, OpenAI's Whisper Large V3.3The distillation process for Whisper is asymmetric, a detail that has profound implications for the GhostType implementation:The Encoder (Frozen Topology): The student retains the entirety of the teacher's encoder. It is topologically identical to Whisper Large V3, possessing 32 transformer blocks, 1280 distinct audio channels (width), and utilizing 128 Mel-frequency bins.5 This design choice is driven by the fact that the encoder is responsible for extracting rich acoustic features from the audio, a process that is difficult to compress without significant accuracy loss. Consequently, the memory footprint and tensor shapes of the encoder in distil-whisper-large-v3 are identical to the massive whisper-large-v3.The Decoder (Aggressive Pruning): The student's decoder is drastically pruned. While the teacher possesses 32 decoder layers, distil-whisper-large-v3 reduces this to just 2 layers.6 This reduction is designed to accelerate the autoregressive generation phase, which is typically the bottleneck in latency-sensitive applications like real-time dictation.For GhostType, this means that while the generation of text will be rapid, the processing of audio (the encoder forward pass) remains computationally heavy, necessitating the optimized tensor operations discussed in Section 4.2.2 The MLX Framework and Unified MemoryThe GhostType application leverages MLX, a framework designed specifically for Apple Silicon (M-series chips). MLX distinguishes itself from PyTorch or TensorFlow through its memory model and operation semantics.7Unified Memory Architecture (UMA): MLX exploits the unified memory of the M-chip, allowing zero-copy operations between the CPU and the GPU (Neural Engine). However, to fully leverage the bandwidth of the UMA, data must be laid out in memory in a specific order that aligns with the hardware's vector processors.Channel-Last Convention: A critical differentiator for MLX is its preference for channel-last layouts (NLC for 1D data, NHWC for 2D data). This contrasts with PyTorch's default preference for channel-first layouts (NCHW). This architectural preference is the root cause of the transposition requirements analyzed in Section 4. If the memory layout does not match the hardware expectation, the framework must perform an implicit (and expensive) permutation, or worse, compute incorrect results by convolving across the wrong dimension.83. Detailed Weight Forensics (Question 1)Query: "What is the exact list of weight keys that DO NOT map to these property paths?"The mismatch between the keys present in the distil-whisper-large-v3 weights.safetensors file and the properties defined in the WhisperModel.swift struct arises from three distinct sources: naming convention divergences, architectural pruning (unused biases), and structural redundancy (weight tying).3.1 The Anatomy of the Source FileThe weights.safetensors file follows the Hugging Face Transformers naming convention.10 For distil-whisper-large-v3, the keys are hierarchically organized under model.encoder and model.decoder. The file is a flat key-value store mapping string names to tensors.To identify the unmapped keys, we must first establish the "correct" mapping. The GhostType architecture typically follows a structure similar to:WhisperModel -> encoder -> blocks -> [Attention, MLP] -> LayerNorm3.2 The "Mapped" Keys ReferenceThe following table establishes the validated mapping for the encoder. Any key in the file not falling into this mapping logic is a candidate for the "Unmapped List."ComponentSource Key (Hugging Face)Target Property (Swift/MLX)Encoder Stemmodel.encoder.conv1.weightencoder.conv1.weightmodel.encoder.conv1.biasencoder.conv1.biasmodel.encoder.conv2.weightencoder.conv2.weightmodel.encoder.conv2.biasencoder.conv2.biasEncoder Blockmodel.encoder.layers.X.self_attn.k_proj.weightFused into blocks[X].attn.weightmodel.encoder.layers.X.self_attn.v_proj.weightFused into blocks[X].attn.weightmodel.encoder.layers.X.self_attn.q_proj.weightFused into blocks[X].attn.weightmodel.encoder.layers.X.self_attn.out_proj.weightblocks[X].attn.out.weightmodel.encoder.layers.X.self_attn_layer_norm.weightblocks[X].attn_ln.weightmodel.encoder.layers.X.fc1.weightblocks[X].mlp1.weightmodel.encoder.layers.X.fc2.weightblocks[X].mlp2.weightmodel.encoder.layers.X.final_layer_norm.weightblocks[X].mlp_ln.weightEncoder Normmodel.encoder.layer_norm.weightencoder.ln_post.weight3.3 The "Exact List" of Unmapped KeysBased on the standard conversion logic from Hugging Face to MLX for Whisper, the following specific keys found in the weights.safetensors file will not map 1:1 to the properties of an inference-optimized WhisperModel class hierarchy.Category A: The "Project Out" RedundancyKey: proj_out.weight (sometimes labeled lm_head.weight in other architectures)Analysis: In the Whisper architecture, the final output projection layer—which maps the hidden state to the vocabulary size (51,866 tokens)—shares its weights with the decoder's input token embedding layer (model.decoder.embed_tokens.weight). This is known as "weight tying."Implication: The safetensors file often includes a separate key for proj_out.weight to satisfy strict loading requirements of some frameworks, or it is simply a duplicate reference. In the GhostType WhisperModel.swift, the code likely uses the token_embedding weights for the final projection. Therefore, the key proj_out.weight is effectively "garbage" during loading because the memory is already populated by model.decoder.embed_tokens.weight.Status: IGNORE / UNMAPPEDCategory B: The Explicitly "Missing" Positional EmbeddingKey: model.encoder.embed_positions.weightAnalysis: As detailed in Section 5, the encoder uses procedural sinusoidal embeddings. However, some conversion scripts (e.g., from transformers) mistakenly save the fixed sinusoidal buffer as a weight tensor in the file to simplify the loading code for generic Transformer classes.Implication: If this key exists in the file, it must be ignored. The AudioEncoder class generates these values mathematically. Overwriting the procedural generator with loaded weights is redundant and potentially dangerous if the file generation used a different frequency base (though unlikely).Status: IGNORE / UNMAPPED (If present)Category C: Fused Attention BiasesKeys:model.encoder.layers.X.self_attn.q_proj.biasmodel.encoder.layers.X.self_attn.k_proj.biasmodel.encoder.layers.X.self_attn.v_proj.biasAnalysis: OpenAI's Whisper implementation uses biases in these linear layers. However, optimized MLX implementations typically fuse the Query, Key, and Value projections into a single c_attn layer (a single Linear layer of size 3 * width).Implication: While the data from these keys is used (concatenated into blocks[X].attn.bias), the keys themselves do not map to a property. WhisperModel likely has a property attn.bias, not attn.q_proj.bias. The loader must manually fuse these three keys into one tensor. Therefore, strictly speaking, the individual keys do not map to a property path; they map to a component of a property path.Category D: Training ArtifactsKeys: model.encoder.layers.X.self_attn.k_proj.weight_scale, ..._inv_scaleAnalysis: If the model was converted from a framework supporting FP8 or Int8 quantization (common with distil-whisper variants optimized for CTranslate2), these metadata keys might persist in the file.12Implication: The standard floating-point inference path in MLX ignores these scaling factors.Status: IGNORE / UNMAPPED3.4 The Deny-List TableThe following table summarizes the keys that the GhostType loader should explicitly ignore or expect to find unmapped.Unmapped Key PatternReasonActionproj_out.weightWeight TyingUse decoder.token_embedding instead.model.encoder.embed_positions.weightProcedural GenerationGenerate via sinusoids() in init.model.decoder.embed_positions.weightException:LOAD THIS. Decoder uses learned embeddings.*.weight_scaleQuantization ArtifactIgnore for FP16/FP32 inference.*.weight_inv_scaleQuantization ArtifactIgnore for FP16/FP32 inference.4. The Tensor Journey: Shape Analysis (Question 2)Query: "What is the correct tensor shape at each step, and is our transpose (0, 2, 1) correct for MLX Conv1d which expects NLC (channels-last)?"This section provides a mathematically rigorous trace of the tensor shapes through the Audio Encoder "Stem," validating the logic in MLXService.swift.4.1 Theoretical PrerequisitesMel Spectrogram: The input to the model is a Log-Mel Spectrogram. For Whisper Large V3 (and thus Distil-Whisper Large V3), this consists of 128 frequency bins (n_mels=128).5Time Resolution: Whisper processes audio in 30-second windows. With a hop size of 160 samples at 16,000 Hz (10ms), this results in 3000 time frames (n_audio_ctx=3000 pre-convolution).1MLX Convention: nn.Conv1d in MLX expects input tensors in the format (Batch, Length, Channels) or NLC.84.2 Step-by-Step Shape CalculusStep 0: The Raw InputThe audio processor generates a flattened array melFlat.Logical Shape: (Batch: 1, Mels: 128, Time: 3000).Code: MLXArray(melFlat).reshaped().Current Tensor: (1, 128, 3000).Context: This is the standard "image-like" representation where height is frequency (Mels) and width is time.Step 1: The Transpose OperationCode: .transposed(0, 2, 1)Operation: Permute axes. Axis 0 (Batch) stays. Axis 1 (128) becomes Axis 2. Axis 2 (3000) becomes Axis 1.Resulting Shape: (1, 3000, 128).Semantic Meaning: (Batch, Length, Channels).Validation: This correctly places the "Time" dimension in the "Length" slot and the "Mel Frequency" dimension in the "Channel" slot.Verdict: The transpose is CORRECT. Without this, the convolution would treat the 3000 time steps as 3000 input channels, and the 128 Mel bins as the sequence length. This would result in a massive parameter mismatch error (expecting kernels of depth 3000 instead of 128) or, if dimensions happened to align, "garbage output".13Step 2: Convolution Layer 1 (conv1)Configuration:in_channels: 128out_channels: 1280 (n_audio_state)kernel_size: 3stride: 1padding: 1Mathematical Operation: 1D Convolution over the Length dimension.Output Length Calculation:$$L_{out} = \lfloor \frac{L_{in} + 2 \times \text{padding} - \text{kernel}}{\text{stride}} \rfloor + 1$$$$L_{out} = \lfloor \frac{3000 + 2(1) - 3}{1} \rfloor + 1 = \lfloor \frac{2999}{1} \rfloor + 1 = 3000$$Resulting Shape: (1, 3000, 1280).Insight: This layer projects the low-dimensional acoustic features (128 Mels) into the high-dimensional latent space of the model (1280 dimensions) while preserving the full temporal resolution.Step 3: Activation 1Operation: GELU (Gaussian Error Linear Unit).Shape: (1, 3000, 1280) (Element-wise, no shape change).Step 4: Convolution Layer 2 (conv2)Configuration:in_channels: 1280out_channels: 1280kernel_size: 3stride: 2padding: 1Output Length Calculation:$$L_{out} = \lfloor \frac{3000 + 2(1) - 3}{2} \rfloor + 1$$$$L_{out} = \lfloor \frac{2999}{2} \rfloor + 1 = 1499 + 1 = 1500$$Resulting Shape: (1, 1500, 1280).Insight: The stride of 2 is the critical "compression" step. It reduces the sequence length by half, effectively compressing 20ms of audio (two 10ms frames) into a single token vector. This explains why the positional embeddings are of length 1500, not 3000.Step 5: Activation 2Operation: GELU.Shape: (1, 1500, 1280).Step 6: Positional Embedding AdditionOperation: x = x + positional_embeddingRequirement: The positional_embedding tensor must be broadcastable to (1, 1500, 1280).Source: Procedurally generated sinusoids (see Section 5).Resulting Shape: (1, 1500, 1280).4.3 Summary of Tensor FlowStageOperationInput Shape (MLX format)Output ShapeNotesInputreshaped(N, 128, 3000)(N, 128, 3000)Assumes NCL sourcePreptranspose(0, 2, 1)(N, 128, 3000)(N, 3000, 128)Critical for MLX Conv1dStem 1conv1 (k=3, s=1)(N, 3000, 128)(N, 3000, 1280)Projects widthAct 1gelu(N, 3000, 1280)(N, 3000, 1280)Stem 2conv2 (k=3, s=2)(N, 3000, 1280)(N, 1500, 1280)Downsamples TimeAct 2gelu(N, 1500, 1280)(N, 1500, 1280)Pos+ pos_embed(N, 1500, 1280)(N, 1500, 1280)Adds sequence info5. Encoder Positional Embedding Deep Dive (Question 3)Query: "Does mlx-community/distil-whisper-large-v3 use learned positional embeddings... or is sinusoidal correct?"This question identifies a common point of failure in Whisper implementations. The answer requires a definitive distinction between the Encoder and the Decoder, and an understanding of the inheritance from the original Whisper architecture.5.1 The Definitive VerdictThe distil-whisper-large-v3 model utilizes:Fixed Sinusoidal Positional Embeddings for the Encoder.Learned Positional Embeddings for the Decoder.The hypothesis that the implementation is "ignoring" a learned encoder embedding key is incorrect. The absence of encoder.positional_embedding in the weights file is the expected behavior for this architecture.5.2 The Evidence TraceInheritance from Whisper: Distil-Whisper is obtained by distilling OpenAI's Whisper. OpenAI's architecture explicitly uses sinusoidal embeddings for the encoder to allow for length extrapolation and to provide a smooth, continuous inductive bias for the acoustic features.1 The student model (Distil-Whisper) copies the teacher's encoder exactly, including this architectural choice.The "Smoking Gun": Your observation that the log shows encoder.conv1.weight but not encoder.positional_embedding confirms the architectural definition. If the embeddings were learned, they would be trainable parameters of shape (1500, 1280), totaling approximately 1.92 million floating-point values. Such a significant parameter set would necessarily be present in the safetensors file to allow the model to function. Its absence confirms it is a computed constant.Hugging Face Config Confirmation: The config.json for distil-whisper-large-v3 does not list encoder_positional_embedding as a learned parameter type, whereas it treats the decoder embeddings as standard lookups.65.3 The Theoretical "Why"Why does Whisper use sinusoidal embeddings for the encoder but learned for the decoder?Encoder (Audio): Audio is a continuous signal where relative position matters more than absolute position. Sinusoidal functions impose a smooth, relative distance metric that generalizes well. Furthermore, it allows the model to potentially handle audio chunks slightly longer than 30s during inference (though the model is trained on 30s) without the hard failure mode of a learned embedding lookup going out of bounds.16Decoder (Text): Text generation is an autoregressive task over discrete tokens. Learned embeddings have been shown to perform slightly better in pure language modeling tasks (like GPT-2/3), capturing semantic nuances of absolute position in a sentence structure.155.4 Implementation Logic for GhostTypeSince the weight file does not contain the key, the AudioEncoder class must generate these embeddings during initialization.Algorithm for Sinusoidal Generation:The embedding $PE_{(pos, 2i)}$ for position $pos$ and dimension $2i$ is:$$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$$$$PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})$$Swift/MLX Implementation Strategy:The loader must be programmed to skip looking for this key for the encoder.Swift// Concept Code
class AudioEncoder: Module {
    let conv1: Conv1d
    let conv2: Conv1d
    let positionalEmbedding: MLXArray // Not a Parameter, but a buffer

    init(config: Config) {
        //... init convolutions...
        // Generate Sinusoids:
        let enc = sinusoids(length: 1500, channels: 1280)
        self.positionalEmbedding = enc.asType(config.dtype) 
        // Note: Do not add to trainable parameters
    }
}
Attempting to load this from the file will result in a "Key not found" error, which is the correct system behavior.6. Implementation Recommendations and OptimizationBased on the forensic analysis, we propose the following specific modifications to the GhostType MLXService and WhisperModel classes to ensure stability and performance.6.1 Robust Weight Loading RoutineTo handle the "keys that do not map" issue gracefully, implement a "strictness" filter in your weight loader.Property-Driven Loading: Do not iterate through the file keys and try to find properties. Instead, iterate through your Model Properties (recursively) and fetch the corresponding key from the file.The "Phantom" Handler: specifically catch the proj_out vs token_embedding case.If the property is decoder.token_embedding, load model.decoder.embed_tokens.weight.If the property is decoder.output_projection, reuse the reference to decoder.token_embedding (if your class separates them).Decoder Embeddings: Ensure decoder.positional_embedding is loaded from model.decoder.embed_positions.weight. This is the most common point of confusion—treating the decoder like the encoder.6.2 Tensor Shape Verification ChecklistInsert assertions in your debug build to verify shapes at runtime. This will catch regression errors if the input preprocessing pipeline changes (e.g., if the audio capture buffer size changes).Assert: melInput.shape ==  (Before transpose)Assert: convInput.shape ==  (After transpose)Assert: conv1Output.shape == Assert: conv2Output.shape == Assert: encoderOutput.shape ==  (After all blocks)6.3 Performance Note: The Transpose CostThe use of transposed(0, 2, 1) is computationally cheap in MLX because it creates a strided view rather than copying data immediately. However, Conv1d operations on non-contiguous memory can be slower because the GPU kernel cannot coalesce memory accesses efficiently.Optimization: In the future, consider modifying the audio feature extractor (the code generating the Mel spectrogram) to output (Time, Channels) or (3000, 128) directly. This would eliminate the need for the transpose and ensure the memory is physically contiguous in the NLC format required by the Neural Engine, potentially shaving milliseconds off the pre-fill latency.7. ConclusionThe integration of distil-whisper-large-v3 into GhostType on macOS requires a precise alignment of tensor shapes and weight mappings that respects the specific architectural divergences of the distilled model and the hardware constraints of Apple Silicon.By confirming the correctness of the (0, 2, 1) transpose, identifying the procedural nature of the encoder's sinusoidal embeddings, and isolating the unmapped "phantom" weights, the engineering team can proceed with confidence. The model's behavior—compressing 3000 frames to 1500 latent vectors and utilizing a frozen, massive encoder paired with a lightweight decoder—is now fully mapped to the Swift implementation.Action Items:Update Loader: Add the "Deny List" for proj_out and encoder.embed_positions.Verify Code: Ensure AudioEncoder generates sinusoids in init().Keep Transpose: Retain the .transposed(0, 2, 1) call in the input pipeline.This concludes the architectural audit.Report prepared by:Lead AI Systems ArchitectGhostType Research DivisionDecember 18, 2025